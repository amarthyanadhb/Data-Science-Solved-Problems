{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, json, random, joblib, re\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "INPUT_PATH = \"/mnt/data/amazonreviews.tsv\"\n",
        "OUT_DIR = \"/mnt/data/amazon_sentiment_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "MNhtnlvyjRll"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure NLTK assets (first run may download)\n",
        "nltk_packages = [\"stopwords\", \"punkt\", \"wordnet\", \"omw-1.4\"]\n",
        "for pkg in nltk_packages:\n",
        "    try:\n",
        "        nltk.data.find(pkg)\n",
        "    except Exception:\n",
        "        nltk.download(pkg)\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMMA = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uwv8RuXjX0y",
        "outputId": "e35e1e44-c519-4475-93bf-c822068fff06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"STEP 1: Loading dataset...\")\n",
        "df = pd.read_csv('amazonreviews.tsv',sep='\\t')\n",
        "print(\"Rows:\", len(df), \"Columns:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYqmY-3tjciT",
        "outputId": "f23b3564-0a64-4f1b-a869-e3ed94194dde"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 10000 Columns: ['label', 'review']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 2: Quick cleaning...\")\n",
        "expected = {'label','review'}\n",
        "if not expected.issubset(df.columns):\n",
        "    raise ValueError(f\"Input must contain columns: {expected}\")\n",
        "\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "df = df.dropna(subset=['label']).reset_index(drop=True)\n",
        "df['review'] = df['review'].fillna(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ginRTnpOk1UV",
        "outputId": "b3df9f05-a052-4463-ea98-b5cde9a3c8b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 2: Quick cleaning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize labels to 0/1\n",
        "df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
        "label_map = {'pos':1,'positive':1,'neg':0,'negative':0,'1':1,'0':0}\n",
        "df['label'] = df['label'].map(label_map)\n",
        "df = df.dropna(subset=['label'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "print(\"After cleaning rows:\", len(df), \"Label counts:\\n\", df['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fub-i-X0k-WA",
        "outputId": "97a3c08d-98e7-4628-954b-b8a2c58251a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After cleaning rows: 10000 Label counts:\n",
            " label\n",
            "0    5097\n",
            "1    4903\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 3: EDA...\")\n",
        "df['review_len'] = df['review'].astype(str).apply(len)\n",
        "print(\"Review length summary:\\n\", df['review_len'].describe())\n",
        "\n",
        "# top tokens sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us2Ki9YUlCTs",
        "outputId": "558279e9-9d76-4d84-d801-3617a9d908ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 3: EDA...\n",
            "Review length summary:\n",
            " count    10000.000000\n",
            "mean       438.695400\n",
            "std        239.241132\n",
            "min        101.000000\n",
            "25%        238.000000\n",
            "50%        391.000000\n",
            "75%        605.000000\n",
            "max       1015.000000\n",
            "Name: review_len, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlZzFQXZlXi8",
        "outputId": "35f1b906-e09c-4b61-e348-e6ae297e0fa1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top tokens sample\n",
        "sample_text = \" \".join(df['review'].astype(str).values[:5000]).lower()\n",
        "sample_text = re.sub(r\"[^a-z0-9\\s']\", \" \", sample_text)\n",
        "tokens = [t for t in nltk.word_tokenize(sample_text) if t not in STOPWORDS]\n",
        "print(\"Top tokens:\", Counter(tokens).most_common(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3IQX_ZFlGLq",
        "outputId": "c623fbe2-967f-451c-84ff-35f0b554e022"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top tokens: [('book', 2952), (\"'s\", 2836), (\"n't\", 2763), ('one', 2046), ('like', 1377), ('great', 1352), ('good', 1348), ('would', 1319), ('read', 1224), ('time', 1003), ('get', 974), ('movie', 969), ('really', 784), ('first', 766), ('much', 709)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wordclouds (save to OUT_DIR)\n",
        "def light_prep(s):\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
        "    return \" \".join([w for w in nltk.word_tokenize(s) if w not in STOPWORDS])\n",
        "\n",
        "pos_text = \" \".join(df[df['label']==1]['review'].astype(str).map(light_prep).values[:5000])\n",
        "neg_text = \" \".join(df[df['label']==0]['review'].astype(str).map(light_prep).values[:5000])\n",
        "WordCloud(width=600, height=300).generate(pos_text).to_file(os.path.join(OUT_DIR, \"wordcloud_pos.png\"))\n",
        "WordCloud(width=600, height=300).generate(neg_text).to_file(os.path.join(OUT_DIR, \"wordcloud_neg.png\"))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6UwEOkalhtz",
        "outputId": "669850da-2029-4aae-e880-5934802e67e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wordcloud.wordcloud.WordCloud at 0x7f4264b4cb90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 4: Preprocessing (tokenize, remove stopwords, lemmatize)...\")\n",
        "def clean_text(s):\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r\"<.*?>\", \" \", s)\n",
        "    s = re.sub(r\"http\\S+|www.\\S+\", \" \", s)\n",
        "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def preprocess_text(s):\n",
        "    s = clean_text(s)\n",
        "    toks = nltk.word_tokenize(s)\n",
        "    toks = [t for t in toks if t not in STOPWORDS]\n",
        "    toks = [LEMMA.lemmatize(t) for t in toks]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "# apply (can be slower on large datasets)\n",
        "df['review_clean'] = df['review'].astype(str).map(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYp7nGqxlL0y",
        "outputId": "52c92053-9fee-4403-dd6b-8ddb7246fac0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 4: Preprocessing (tokenize, remove stopwords, lemmatize)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 5: Train/test split...\")\n",
        "X = df['review_clean'].values\n",
        "y = df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=SEED)\n",
        "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KY0lqZl0VN",
        "outputId": "87221589-6a86-4fc9-cab0-ff483a4f1502"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 5: Train/test split...\n",
            "Train size: 8000 Test size: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 6: TF-IDF vectorizer...\")\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hywAb_fl6HC",
        "outputId": "6b919520-8d0e-439f-f0d8-91b6fd0e9fe6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 6: TF-IDF vectorizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 7: Build pipelines (Logistic, SVM, MLP)...\")\n",
        "pipelines = {\n",
        "    \"logistic\": Pipeline([(\"tfidf\", tfidf), (\"clf\", LogisticRegression(max_iter=1000, random_state=SEED))]),\n",
        "    \"svm\": Pipeline([(\"tfidf\", tfidf), (\"clf\", LinearSVC(max_iter=10000, random_state=SEED))]),\n",
        "    \"mlp\": Pipeline([(\"tfidf\", tfidf), (\"clf\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=50, random_state=SEED))])\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ2DEW0DmAQI",
        "outputId": "bfd1482c-828a-47b6-b10c-03e26f056bc7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 7: Build pipelines (Logistic, SVM, MLP)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- STEP 8: Train & evaluate -----------------\n",
        "print(\"STEP 8: Train & evaluate models...\")\n",
        "results = {}\n",
        "for name, pipe in pipelines.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"{name} -> acc:{acc:.4f} f1:{f1:.4f} prec:{prec:.4f} rec:{rec:.4f}\")\n",
        "    results[name] = {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"f1\": float(f1),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"confusion_matrix\": cm.tolist()\n",
        "    }\n",
        "    joblib.dump(pipe, os.path.join(OUT_DIR, f\"{name}_pipeline.joblib\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7PVJ5rcmM7N",
        "outputId": "eb16af4f-f087-4887-8d28-8428240d56c7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 8: Train & evaluate models...\n",
            "Training logistic...\n",
            "logistic -> acc:0.8510 f1:0.8480 prec:0.8488 rec:0.8471\n",
            "Training svm...\n",
            "svm -> acc:0.8430 f1:0.8403 prec:0.8386 rec:0.8420\n",
            "Training mlp...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mlp -> acc:0.8315 f1:0.8294 prec:0.8239 rec:0.8349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation\n",
        "print(\"STEP 9: 5-fold CV on training set...\")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "cv_summary = {}\n",
        "for name, pipe in pipelines.items():\n",
        "    print(\"CV:\", name)\n",
        "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
        "    cv_summary[name] = {\"f1_mean\": float(scores.mean()), \"f1_std\": float(scores.std()), \"folds\": [float(s) for s in scores.tolist()]}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FimK_eF7maIo",
        "outputId": "31972125-a086-49f8-8e49-2011d43c948b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 9: 5-fold CV on training set...\n",
            "CV: logistic\n",
            "CV: svm\n",
            "CV: mlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- STEP 10: Save metrics & outputs -----------------\n",
        "print(\"STEP 10: Save metrics & artifacts...\")\n",
        "def make_json_serializable(o):\n",
        "    if isinstance(o, dict):\n",
        "        return {k: make_json_serializable(v) for k,v in o.items()}\n",
        "    if isinstance(o, list):\n",
        "        return [make_json_serializable(v) for v in o]\n",
        "    if isinstance(o, np.generic):\n",
        "        return o.item()\n",
        "    return o\n",
        "\n",
        "out_metrics = {\n",
        "    \"results_test\": results,\n",
        "    \"cv_summary\": cv_summary,\n",
        "    \"n_rows\": int(len(df)),\n",
        "    \"train_size\": int(len(X_train)),\n",
        "    \"test_size\": int(len(X_test))\n",
        "}\n",
        "out_metrics = make_json_serializable(out_metrics)\n",
        "with open(os.path.join(OUT_DIR, \"metrics_summary.json\"), \"w\") as f:\n",
        "    json.dump(out_metrics, f, indent=2)\n",
        "\n",
        "# save confusion matrices images\n",
        "for name, info in results.items():\n",
        "    cm = np.array(info['confusion_matrix'])\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['neg','pos'], yticklabels=['neg','pos'])\n",
        "    plt.title(f\"Confusion - {name}\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, f\"confusion_{name}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "print(\"Done. Artifacts saved to:\", OUT_DIR)\n",
        "print(\"Examples: models (*.joblib), wordclouds, confusion matrices, metrics_summary.json\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHSY8Co9m9lb",
        "outputId": "2ea1ec4f-4799-4634-f4b7-75ec017df959"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 10: Save metrics & artifacts...\n",
            "Done. Artifacts saved to: /mnt/data/amazon_sentiment_outputs\n",
            "Examples: models (*.joblib), wordclouds, confusion matrices, metrics_summary.json\n"
          ]
        }
      ]
    }
  ]
}